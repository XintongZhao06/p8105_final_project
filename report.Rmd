---
title: "Short-Form Video Analytics: Completion Rate and Trend Prediction"
output:
  html_document:
    theme: flatly
    highlight: textmate
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  warning = FALSE, 
  message = FALSE,
  fig.align = "center"
)

install.packages(c(
  "tidyverse",
  "modelr",
  "mgcv",
  "broom",
  "kableExtra"
), repos = "https://cran.rstudio.com/")


library(knitr)
library(kableExtra)
library(tidyverse)
```

# **0. Abstract**

This project investigates two key aspects of short-form video performance: **completion rate** (viewer retention) and **trend label prediction** (video popularity level). 

Using a curated dataset of nearly 50,000 TikTok and YouTube Shorts videos, we:

1. **Conduct extensive exploratory analysis** to characterize distributions, temporal patterns, and content-level differences
2. **Develop regression models** examining metadata features—duration, engagement metrics, creator tier, and stylistic attributes—and their statistical associations with completion rate
3. **Build machine learning models** to predict trend labels constructed from weighted engagement scores

## **0.1 Key Findings**

**Completion Rate Analysis:** Duration emerges as the strongest correlate of retention, while overall explanatory power remains low due to unobserved creative and algorithmic factors.

**Trend Prediction:** Gradient Boosting yields the highest performance across accuracy, F1, and cross-validation metrics, successfully distinguishing low, medium, and high-trend content through platform–tier dynamics, regional differences, and engagement velocity.

## **0.2 Contributions**

This project offers a dual contribution:
- **Descriptive insights** into structural factors influencing viewer retention
- **A practical framework** for predicting video popularity using platform metadata

Limitations related to metadata coverage, content heterogeneity, and synthetic data generation are discussed, along with directions for future multimodal and time-aware modeling.

# **1. Motivation**

Short-form platforms such as TikTok and YouTube Shorts generate massive volumes of user interaction data. Understanding patterns that drive **completion rate** (viewer retention) and **trend label** (video popularity) provides insight into content performance and optimization strategies.

As competition for user attention intensifies and platform algorithms become increasingly opaque, identifying measurable, metadata-based signals of performance becomes both practically valuable and analytically challenging.

## **1.1 Research Questions**

Our project is motivated by two central questions:

### **1.1.1 Which video attributes show statistical associations with completion rate?**

Completion rate is one of the most direct indicators of viewer interest, yet it is also shaped by unobserved content qualities such as:
- Pacing and narrative structure
- Editing style and visual quality
- Emotional engagement
- Algorithmic exposure patterns

By examining observable metadata—including duration, engagement metrics, posting timing, and creator attributes—we aim to understand which elements have consistent and interpretable associations with retention. 

> **Note:** Rather than predicting retention with high accuracy, our goal is to clarify how and why certain structural attributes relate to viewer behavior.

### **1.1.2 Can trend labels be predicted from metadata using machine learning?**

Emerging trends on short-form platforms are influenced by:
- Social contagion dynamics
- Algorithmic amplification
- Cross-platform cultural patterns

Trend labels, constructed from multi-dimensional engagement metrics, provide an alternative perspective on video performance that may be more predictable than completion rate. This motivates our second line of inquiry: whether machine-learning models can effectively classify videos into **Low, Medium, and High Trend** categories using platform- and creator-level metadata, interaction features, and designed engagement metrics.

## **1.2 Methodological Approach**

Beyond these two questions, the project is guided by a broader methodological motivation: **to understand the limits and possibilities of metadata analytics** in a domain dominated by creative expression, algorithmic influence, and rapidly evolving user behavior. 

### **1.2.1 Why Completion Rate is Challenging**

Completion rate, by its nature, is tightly linked to content quality—which is not directly observable—so we anticipate low explanatory power from statistical models. This expectation shapes our approach: instead of treating prediction as the primary objective, we emphasize:
- Descriptive modeling
- Structural pattern identification
- Model interpretability

### **1.2.2 Why Trend Prediction May Work Better**

The shift to trend prediction acknowledges that some aspects of video popularity may be more amenable to machine learning. Engagement-driven trend labels:
- Capture richer behavioral signals than completion rate alone
- May reveal platform-level dynamics that metadata can meaningfully approximate

## **1.3 Project Vision**

Overall, the project is driven by a desire to **bridge interpretability and predictability**, examining what metadata can reveal about short-form video performance—and where its limitations begin.

# **2. Related Work**

This project focuses on completion rate analysis and trend prediction for short-form videos, drawing inspiration from practical research in the short-form video field, industry reports, and class discussions.

## **2.1 Academic Research Foundations**

### **2.1.1 Short-Video Engagement Dynamics**

Key insights include studies on TikTok and YouTube Shorts performance, which identify **video duration** and **upload timing** as critical factors influencing completion rates. These findings guided the focus on temporal patterns and content characteristics in our Exploratory Data Analysis (EDA).

### **2.1.2 Engagement Rate Distributions**

Research on short-video metrics notes that engagement rates exhibit **skewed distributions**, consistent with our observation of right-skewed engagement rates. This supports the use of **logarithmic transformations** for modeling.

## **2.2 Industry Resources**

Industry resources played a vital role in shaping our approach:

- **Short-form video datasets** from Kaggle provided structural templates
- **Platform-specific reports** (e.g., TikTok Creator Trend Reports) informed feature selection, such as:
  - Creator tiers
  - Hashtag performance metrics
  - Platform-specific engagement patterns

## **2.3 Methodological Guidance from Coursework**

### **2.3.1 Handling Imbalanced Data**

Class discussions on predictive modeling for imbalanced data led to the adoption of:
- **Weighted F1-scores** for evaluation
- **Stratified cross-validation** to address uneven distribution of trend labels (Low/Medium/High)

### **2.3.2 EDA Best Practices**

Courses on EDA best practices guided the development of a systematic analytical framework, prioritizing:
1. Analysis of key metrics
2. Temporal pattern identification
3. Platform difference characterization
4. Model development informed by exploratory insights

## **2.4 Project Evolution**

### **2.4.1 Initial Focus: Completion Rate Prediction**

The project initially planned to focus solely on completion rate prediction using standard regression and machine learning approaches.

### **2.4.2 Pivot to Multi-Metric Analysis**

This adjustment stemmed from insights from **multi-metric engagement scoring frameworks**—combining comments, shares, and views provides a more comprehensive reflection of content popularity than completion rate alone.

### **2.4.3 Platform-Specific Behavior**

Research on platform-specific user behavior led to the addition of **interaction features** (e.g., platform-creator tier combinations) in feature engineering, as engagement drivers differ between platforms like TikTok and YouTube Shorts.

## **2.5 Integration of External Experience**

These adjustments are based on the integration of external experience, aiming to enhance the project's:
- **Practical applicability** for content creators and platforms
- **Predictive accuracy** through informed feature engineering
- **Analytical rigor** through methodologically sound approaches

# **3. Initial Questions and Project Evolution**

## **3.1 Starting Point: Focus on Completion Rate**

The project began with an interest in understanding viewer retention by examining completion rate. Our initial plan included:

1. **Exploratory data analysis** to identify patterns and relationships
2. **Regression modeling** to quantify associations between metadata and retention
3. **Machine learning models** to evaluate predictive performance

## **3.2 The Reality Check: Low Predictability**

Early results revealed a critical finding: **completion rate was extremely difficult to predict**, even when more flexible machine learning models were applied. 

This outcome suggested that completion behavior is strongly driven by **unobserved factors** such as:
- Creative quality and production value
- Narrative pacing and emotional resonance
- Visual aesthetics and editing techniques
- Algorithmic recommendation exposure
- Individual viewer preferences and context

These elements are not present in metadata and fundamentally limit predictive power.

## **3.3 Strategic Pivot: From Prediction to Description**

As a result, completion rate was treated **only as a descriptive target**, and the focus shifted to:
- **Exploratory identification** of structural associations
- **Interpretable quantification** of relationships between observable features and retention
- **Understanding limitations** of metadata-based modeling

Rather than forcing predictive accuracy, we embraced a more modest and scientifically appropriate goal: **understanding what metadata can and cannot tell us** about viewer behavior.

## **3.4 Expanding Scope: The Search for Predictable Targets**

These findings prompted us to adjust the scope of our research. Since metadata did not explain view completion rates well, we began to consider whether **other performance metrics were more suitable for prediction**.

**Trend labels** emerged as a suitable alternative because they:

1. **Capture platform-level dynamics** rather than individual viewing decisions
2. **Rely on multiple engagement signals** (comments, shares, saves, views) rather than a single metric
3. **May be less dependent** on unobserved content attributes
4. **Reflect cumulative social behavior** that metadata might better approximate

## **3.5 The Second Research Focus**

This consideration led to the second part of the project, which examines **the extent to which trend labels can be predicted using machine learning methods**.

## **3.6 Reflection on Evolution**

The evolution of these research questions reflects a growing understanding of both:
- **The strengths of metadata analytics**: identifying structural patterns and platform differences
- **The limits of metadata analytics**: capturing creative quality and individual viewer behavior

This adaptive approach demonstrates scientific maturity—recognizing when initial hypotheses need revision and pivoting to more tractable and meaningful research questions.

# **4 Data**

## **4.1 Source Information**

- **Platform**: [Kaggle](https://www.kaggle.com/datasets/tarekmasryo/youtube-shorts-and-tiktok-trends-2025)

- **Dataset Type**: Social media analytics, synthetic/curated dataset

- **Size**: ~50,000 video records

- **Time Period**: 2025 (January-September)

## **4.2 Key Variables**

### **4.2.1 Engagement Metrics**

- **views**: Total views count

- **likes**: Likes count

- **comments**: Comments count

- **shares**: Shares count

- **saves**: Saves count

- **completion_rate**: `avg_watch_time_sec / duration_sec`

- **engagement_rate**: `(likes+comments+shares+saves) / views`

### **4.2.2 Classification & Target Variables**

- **completion_rate**

- **trend_label**

## **4.3 Data Characteristics**

### **4.3.1 Data Transformations**

- Categorical variables converted to factors

- Derived metrics

### **4.3.2 Data Cleaning**

There are **no missing values** in the dataset, so the data cleaning process is not included.

# **5. Exploratory Data Analysis (EDA)**

## **5.1 EDA — Completion Rate**

### **5.1.1 Overview and Objectives**

This section aims to analyze the distribution characteristics of completion rates and their relationships with key variables, laying the groundwork for subsequent predictive modeling.

### **5.1.2 Data Preprocessing**

During data preprocessing, we:
- Selected **18 core features** from the raw dataset:
  - Engagement metrics (likes, comments, shares, saves)
  - Temporal features (upload hour, day of week)
  - Content attributes (duration, category, title characteristics)
  - Creator attributes (tier, average views)
- Converted categorical variables to appropriate formats
- Handled missing values (none found)
- Produced a cleaned dataset of **approximately 48,000 records** with no missing values

### **5.1.3 Descriptive Statistics**

Statistical results showed:
- **Average completion rate**: 0.638 (median: 0.642)
- **Average views**: 128,000
- **Average engagement rate**: 0.067

These values indicate **moderate audience retention** and **low-to-moderate interaction** overall.

### **5.1.4 Target Variable Distribution**

#### **5.1.4.1 Completion Rate: Symmetric and Stable**

Analysis revealed that completion rates exhibit a **roughly symmetric distribution**, concentrated between 0.6 and 0.7, with:
- No significant skewness
- No heavy tails
- A stable target for modeling

#### **5.1.4.2 Engagement Rate: Right-Skewed with Outliers**

In contrast, engagement rates show a **severe right-skewed distribution**, with:
- Most values below 0.10
- Only a few high-engagement outliers
- Wide variability visible in boxplot comparisons

### **5.1.5 Relationship Between Completion and Engagement**

A scatter plot revealed a **weak positive correlation** (Pearson's r = 0.12) between completion rate and engagement rate.

**Interpretation:**
- High retention does not imply high interaction
- The two metrics reflect different aspects of viewer behavior:
  - **Completion rate**: Passive retention and content watchability
  - **Engagement rate**: Active interaction and content shareability
- Different factors may drive each outcome

This provides justification for **treating them as independent outcome variables** in our analysis.

### **5.1.6 Temporal Patterns**

**Upload Volume by Hour:**
- Peak upload times: **16:00–22:00** (evening hours)
- Aligns with users' active periods after work/school

**Completion Rate by Hour:**
- Average completion rates remain **stable across all time slots** (0.635–0.645)
- No significant variation by upload hour

**Weekday vs. Weekend:**
- Weekday uploads: 29,953 videos
- Weekend uploads: 18,126 videos
- Completion rates show **almost no difference** (0.638 vs. 0.636)

**Key Finding:** This result contradicts the initial hypothesis that "timing affects retention," indicating that short-form video viewers' watching behavior **is not restricted by upload timing**.

### **5.1.7 Content Characteristics**

#### **5.1.7.1 Video Duration Distribution**

Video duration follows a **right-skewed distribution**:
- **78% of videos** are shorter than 50 seconds
- Consistent with short-form video platform norms

#### **5.1.7.2 Duration and Completion Rate Relationship**

A **non-linear relationship** exists between duration and completion rate:
- Completion rates **peak at approximately 12 seconds** (0.68)
- Rates **decline steadily** as duration increases
- Videos longer than 120 seconds drop to **0.52 completion rate**

**Duration Intervals Created:**
- <15s
- 15-30s
- 30-60s
- >60s

These intervals facilitate subsequent group analysis.

### **5.1.8 Title Characteristics**

#### **5.1.8.1 Title Length**

Title lengths are concentrated between **10–40 characters**, with:
- No consistent monotonic relationship between length and completion rate
- Suggests length alone is not a strong predictor

#### **5.1.8.2 Emoji Usage: A Significant Factor**

**Videos with emojis in titles show:**
- **Average completion rate**: 0.659
- **3.2% higher** than videos without emojis (0.627)
- **Engagement rate 41% higher** (0.095 vs. 0.067)

**Additional observations:**
- Higher medians for both completion and engagement rates
- Fewer low-performance outliers

**Interpretation:** Emojis act as **visual cues** that enhance initial audience interest and retention.

### **5.1.9 Category Analysis**

#### **5.1.9.1 Completion Rate by Content Category**

Completion rates vary significantly by content category:

**Highest performers:**
- **Music**: ~0.67 median
- **Fitness**: ~0.67 median

**Lowest performers:**
- **Education**: ~0.59 median
- **Technology**: ~0.59 median

#### **5.1.9.2 Duration-Category Interaction**

When stratified by duration and category:
- **Music videos** maintain relatively high completion rates even in longer intervals (30–60s: 0.64)
- **Education videos** see a sharp drop to 0.51 in the same interval

**Key Insight:** Category-specific content quality moderates the impact of duration on retention:
- **Music**: High appeal maintains engagement
- **Education**: Information density may create fatigue

## **5.2 EDA — Trend Label**

### **5.2.1 Motivation for Trend Labels**

To go beyond single-metric limitations and comprehensively capture content popularity, we constructed an **"engagement score"** by integrating multiple engagement-related features, then generated trend labels.

### **5.2.2 Engagement Score Construction**

The engagement score formula:

```
Engagement Score = 
  Comment Rate (20% weight) +
  Share Rate (30% weight) +
  Views (20% weight) +
  Text Richness (10% weight) +
  Weekend Effects (10% weight) +
  Creator Tier Bonuses (0–0.5)
```

### **5.2.3 Trend Label Categories**

Based on engagement scores, videos were categorized into three trend labels:

| Trend Label | Engagement Score | Distribution |
|-------------|------------------|--------------|
| **Low Trend** | ≤ 0.8 | 42% |
| **Medium Trend** | 0.8 < score ≤ 1.5 | 47% |
| **High Trend** | > 1.5 | 11% |

The distribution shows **mild imbalance** with more Low Trend content.

### **5.2.4 Engagement Score Distribution by Trend Label**

The distribution shows **clear differentiation**:
- **Low Trend**: 0.6–0.8
- **Medium Trend**: 1.0–1.5
- **High Trend**: 1.8–3.0

**Low overlap** between categories validates the effectiveness of trend labels in stratifying content popularity.

### **5.2.5 Platform Comparison**

#### **5.2.5.1 TikTok vs. YouTube Shorts**

TikTok videos have **higher median engagement scores** than YouTube Shorts across all trend labels:

**Example (High Trend):**
- TikTok: 2.3
- YouTube: 1.9

**Interpretation:** TikTok's algorithm or user base is **more likely to drive high-interaction content**.

#### **5.2.5.2 Device Type: No Significant Impact**

Mobile and desktop users show **nearly identical median scores** for each trend label.

**Key Finding:** Content popularity—rather than viewing device—determines interaction levels.

### **5.2.6 Core Metrics by Trend Label**

Boxplots reveal a **strong monotonic relationship**: views, likes, comments, shares, and saves all increase significantly with trend label level.

| Metric | High Trend | Low Trend | Ratio |
|--------|-----------|-----------|-------|
| **Median Views** | 380,000 | 45,000 | 8.4× |
| **Median Likes** | 18,000 | 1,200 | 15× |
| **Engagement Rate** | 0.145 | 0.032 | 4.5× |

**Confirmation:** Trend labels effectively aggregate multi-dimensional popularity metrics.

### **5.2.7 Creator Analysis**

There is a **positive correlation** (r = 0.48) between a creator's average views and individual video views, but with **high dispersion**.

Star creators have a **median engagement score 3.2× that of Micro creators**, reflecting the **"influence effect"**:
- Larger audience base provides initial exposure
- Creator credibility enhances audience trust

**High dispersion** indicates:
- Micro creators can still produce viral videos
- Star creators may have underperforming content

**Conclusion:** Creator tier is an important predictive factor, but **content-specific features remain critical**.

### **5.2.8 Hashtag Analysis**

Popular hashtags like:
- **#FYP** (For You Page)
- **#GRWM** (Get Ready With Me)

These drive **billions of views** and serve as key channels for content discovery.

Niche hashtags like:
- **#BookTok** (book-related)
- **#StudyWithMe** (study-along)

These have:
- Lower view counts
- Precise, engaged audiences

**Strategic Insight:** Creators should **combine popular and niche hashtags** to balance exposure and audience relevance.

### **5.2.9 Platform Differences in Trend Distribution**

Significant differences exist in trend label distribution:

| Platform | High Trend | Low Trend |
|----------|-----------|-----------|
| **TikTok** | 13% | 38% |
| **YouTube Shorts** | 8% | 47% |

- **TikTok**: More right-skewed with more high-engagement outliers
- **YouTube Shorts**: More clustered at low levels

This reflects that:
- **TikTok's algorithm** (e.g., For You Page focus on novelty and popularity) is more likely to drive content virality
- **YouTube Shorts** may prioritize established creators or longer content

## **5.3 Group Comparison Analysis**

This section examines how completion rates vary across different content categories and duration bins, exploring interaction effects and nonlinear patterns that inform subsequent modeling decisions.

### **5.3.1 Completion Rate by Content Category**

To evaluate whether completion rate varies across content types, we compared distributions using **boxplots with jittered points**.

Results show distinct patterns across categories:

**High-Performing Categories:**
- **Music** and **Fitness** videos exhibit:
  - Higher median completion rates
  - Tighter distributions (less variability)
  - More consistent viewer retention

**Lower-Performing Categories:**
- **Education** and **Technology** videos display:
  - Lower central values
  - Wider spread (higher variability)
  - Greater difficulty retaining viewers

These findings align with earlier observations that **entertainment-oriented categories** tend to perform better on passive engagement metrics such as completion:
- Entertainment content requires less cognitive effort
- Information-heavy content may fatigue viewers
- Category type fundamentally shapes retention potential

### **5.3.2 Completion Rate Across Duration Bins**

We examined completion rate across **four duration bins**:
- <15s
- 15–30s
- 30–60s
- >60s

The boxplots reveal a **clear pattern**:

| Duration Bin | Retention Level |
|--------------|----------------|
| **<15s** | Highest retention |
| **15–30s** | Moderate retention |
| **30–60s** | Lower retention |
| **>60s** | Significantly lowest retention |

The **steady decline across bins** aligns with the nonlinear patterns identified earlier, confirming that:
- Viewers on short-form platforms **strongly prefer brief, quickly consumable content**
- Each additional duration interval corresponds to measurable retention loss
- Platform users exhibit limited patience for longer content

**Conclusion:** This bin-based comparison reinforces **duration as one of the dominant structural factors** influencing retention.

### **5.3.3 Duration–Category Interaction**

To explore whether the **effect of duration varies by content type**, we visualized completion rates using a **violin plot stratified by category and duration bin**.

Results show important differences:

**Music Videos:**
- Sustain **relatively high completion rates** even in longer bins
- Demonstrate resilience to duration increases
- Suggest that engaging audio content maintains attention

**Lifestyle Videos:**
- Show **moderate declines** as duration increases
- Mid-range sensitivity to length

**Education Videos:**
- Exhibit **sharper declines** as duration increases
- Most sensitive to duration effects
- Information density may create cognitive fatigue

These patterns indicate that **duration does not operate uniformly across categories**:
- Content type **moderates the magnitude** of duration's impact
- The relationship between length and retention is **category-dependent**
- Different content types require different optimization strategies

**Modeling Implication:** This interaction supports the inclusion of **category–duration interaction features** in subsequent modeling.

### **5.3.4 Completion Rate vs. Duration (LOESS Curve)**

A **scatterplot with global LOESS smoothing** provides a continuous view of the duration–completion relationship.

#### **5.3.4.1 Nonlinear Decline Pattern**

The curve shows a **pronounced nonlinear pattern**:

1. **Sharp Initial Decline:**
   - Retention decreases rapidly as duration increases from very short lengths
   - Steepest drop occurs in the first 20-30 seconds

2. **Gradual Leveling:**
   - The decline gradually levels off at longer durations
   - Suggests a "floor effect" where remaining viewers are committed

#### **5.3.4.2 Methodological Validation**

This smooth trend:
- **Confirms earlier bin-based findings** with greater precision
- Reveals **more nuanced curve behavior** across the full duration range
- Shows that the relationship is continuously nonlinear, not just step-wise

**Modeling Motivation:** The result provides **empirical justification** for employing nonlinear functional forms (such as GAM) in the regression models developed in Section 6.

### **5.3.5 LOESS-Based Optimal Duration**

A separate LOESS model was used to estimate the **duration at which predicted completion rate reaches its maximum**.

#### **5.3.5.1 The Optimal Range**

The resulting curve **peaks at approximately 5–10 seconds**, after which retention declines consistently.

#### **5.3.5.2 Interpretation and Caveats**

While this peak is **descriptive rather than prescriptive**, it highlights that:
- **Ultra-short videos** are most likely to be completed
- The "sweet spot" exists in the very brief range
- Platform design favors extremely concise content

**Important Note:** This optimal duration reflects completion rate, not necessarily engagement quality or business value.

#### **5.2.5.3 Cross-Category Validation**

A **faceted boxplot by category and duration** further supports this pattern by showing that:
- **Virtually all categories** experience reduced retention beyond short durations
- The **severity varies by content type**:
  - Music maintains retention better at longer durations
  - Education shows steeper declines

**Universal Pattern:** Regardless of category, brevity strongly predicts completion.

# **6 Detailed Regression & ML Analysis**

## **6.1 Topic 1: Completion Rate Analysis**

### **6.1.1 Purpose of the Regression Analysis**

Completion rate serves as a key performance metric for short-form videos, reflecting how fully viewers watch a given video. Although creators often attribute retention to content quality, narrative structure, or emotional engagement, these elements are unobserved in platform metadata. Our modeling objective is therefore not prediction, but descriptive quantification of associations between completion_rate and observable characteristics such as duration, engagement metrics, posting timing, and creator attributes.

Given the inherent limitations of metadata and the complexity of viewer behavior — shaped by recommendation algorithms, scrolling patterns, and individual preferences — we expect the explanatory power of the regression models to be modest. Nonetheless, regression analysis still offers a clear way to assess which metadata features are associated with completion rate and to identify where nonlinear or structural patterns may warrant more flexible models.

### **6.1.2 Linear Regression Models**

We first developed three linear models to examine these associations: (1) a baseline regression including only timing-related features; (2) an engagement-augmented model that incorporates log-transformed interaction metrics; and (3) a full specification that adds creator-level attributes and applies backward stepwise selection to identify a more parsimonious set of predictors. This progression allowed us to evaluate how the explanatory contribution of different variable groups changes as the model becomes more comprehensive. We then extended the analysis to nonlinear GAM models to assess whether more flexible functional forms capture additional structure or improve explanatory performance beyond what linear models can provide.

#### **6.1.2.1 Baseline Linear Regression**

The baseline regression evaluates how timing-related features and basic video attributes relate to completion_rate. The estimated model is:

$$
\begin{aligned}
\text{completion_rate}_i 
&= \beta_0 
+ \beta_1 \,\text{event_season}_i
+ \beta_2 \,\text{is_weekend}_i \\
&\quad + \beta_3 \,\text{duration_sec}_i
+ \beta_4 \,\text{upload_hour}_i
+ \varepsilon_i .
\end{aligned}
$$


<div style="text-align: center;">
  <img src="regression_images/model1.png" width="100%">
 <p>Table 6.1&nbsp;&nbsp;&nbsp;&nbsp;Summary of baseline linear regression</p>
</div>

Table 6.1 reports the estimated coefficients and summarizes how basic timing and structural attributes relate to completion rate. The intercept reflects the expected completion rate for the reference category, and while not substantively meaningful on its own, it serves as the baseline against which other coefficients are interpreted. Consistent with expectations, video duration emerges as the strongest predictor: the coefficient for `duration_sec` is negative ($\beta_3$ = -0.0009) and highly significant (p-value < 0.05), indicating that longer videos tend to yield lower completion rates, even within the short-form format. Among the seasonal indicators, only the Regular season shows a small positive association (p-value = 0.041 < 0.05). The remaining `event_season` categories, as well as `is_weekend` and `upload_hour`, do not exhibit statistically significant effects, suggesting that broad posting time patterns have limited influence on completion rate in this dataset.

Overall, the baseline model highlights duration as the primary structural correlate of viewer retention, while other timing-related features contribute little explanatory power. This motivates the incorporation of engagement and creator-level variables in subsequent models.

#### **6.1.2.2 Engagement-Augmented Linear Regression**

To explore whether audience interaction patterns co-vary with viewer retention, the second model adds log-transformed engagement metrics — likes, comments, shares, and saves — to the baseline specification. These variables allow us to examine the relationship between video engagement signals and completion rates. The augmented model is presented as follows:

<span style="font-size: 95%;">
$$
\begin{aligned}
\text{completion_rate}_i 
&= \beta_0 
+ \beta_1 \,*\text{event_season}_i
+ \beta_2 \,*\text{is_weekend}_i
+ \beta_3 \,*\text{duration_sec}_i \\
&\quad + \beta_4 \,*\text{upload_hour}_i + \beta_5 *\log(\text{likes}_i + 1)
+ \beta_6 *\log(\text{comments}_i + 1) \\
&\quad + \beta_7 *\log(\text{shares}_i + 1)
+ \beta_8 *\log(\text{saves}_i + 1)
+ \varepsilon_i .
\end{aligned}
$$
</span>

<div style="text-align: center;"> 
<img src="regression_images/model2.png" width="100%"> 
<p>Table 6.2&nbsp;&nbsp;&nbsp;&nbsp;Summary of engagement-augmented linear regression</p> 
</div>

Table 6.2 presents the augmented model. Duration remains the strongest negative predictor, consistent the baseline finding. Among engagement metrics, `log_likes` and `log_saves` show statistically significant positive associations with `completion_rate` (p < 0.05). This pattern indicates that videos with stronger engagement signals also tend to have higher completion rates, although the association is correlational rather than causal. In contrast, `log_comments` and `log_shares` are not significant, indicating that not all engagement forms are linked to retention. The timing-related predictors (`event_season`, `is_weekend`, and `upload_hour`) continue to show limited explanatory value, reinforcing that temporal posting patterns do not strongly shape viewer retention.

Overall, expanding the model to incorporate interaction measures offers a modest improvement but does not fundamentally change the structure of associations identified earlier.

#### **6.1.2.3 Full Linear Model and Stepwise Selection**

To examine whether creator attributes or stylistic choices add explanatory value beyond engagement patterns, the third model introduces creator-level variables such as log_creator_avg, creator_tier, and has_emoji. We define the full model as follows:

<span style="font-size: 95%;">
$$
\begin{aligned}
\text{completion_rate}_i
&= \beta_0
+ \beta_1 \,*\text{event_season}_i
+ \beta_2 \,*\text{is_weekend}_i
+ \beta_3 \,*\text{duration_sec}_i
+ \beta_4 \,*\text{upload_hour}_i \\
&\quad + \beta_5 *\log(\text{likes}_i + 1)
+ \beta_6 *\log(\text{comments}_i + 1)
+ \beta_7 *\log(\text{shares}_i + 1)
+ \beta_8 *\log(\text{saves}_i + 1) \\
&\quad + \beta_9 *\log(\text{creator_avg_views}_i + 1)
+ \beta_{10} *\text{creator_tier}_i
+ \beta_{11} *\text{has_emoji}_i
+ \varepsilon_i .
\end{aligned}
$$
</span>

Because the full specification contains numerous correlated predictors, a backward stepwise procedure is applied to identify a more concise subset of variables. The results of final model are presented in Table 6.3.

<div style="text-align: center;"> 
<img src="regression_images/model3.png" width="100%"> 
<p>Table 6.3&nbsp;&nbsp;&nbsp;&nbsp;Summary of stepwise (backward) regression</p> 
</div>


<span style="font-size: 95%;">
$$
\begin{aligned}
\widehat{\text{completion_rate}}_i
&= \ 0.6638
- 0.0007932 \,*\text{duration_sec}_i
+ 0.008618 *\log(\text{likes}_i + 1) \\
&\quad + 0.002520 *\log(\text{saves}_i + 1)
- 0.008323 *\log(\text{creator_avg_views}_i + 1) \\
&\quad + 0.02303\, *\text{has_emoji}_i .
\end{aligned}
$$
</span>

The backward stepwise model finally retains only five predictors — `duration_sec`, `log_likes`, `log_saves`, `log_creator_avg`, and `has_emoji` — indicating that these features are the ones most strongly associated with variation in completion rate. The negative coefficient for `log_creator_avg` is particularly notable. Holding other factors constant, videos from creators with larger average viewership tend to show slightly lower completion rates. This may reflect heterogeneous audience behavior or differences in content style across creator tiers. Meanwhile, `has_emoji` shows a small but statistically significant positive effect, suggesting that visual cues in titles may enhance viewer engagement. Variables removed during stepwise selection, such as the seasonal and posting-time indicators, contributed little once the more informative predictors were included. 

Overall, the stepwise model produces a concise set of variables strongly linked to completion rate. This provides a clear summary of the linear relationships, establishing a useful baseline for further exploration.

#### **6.1.2.4 Comparison of Linear Models**

The three linear models show small but incremental improvements in fit as additional predictors are added. As shown in Table 6.4, the baseline model explains only a small share of variation in `completion_rate` (r-squared = 0.0183). Adding engagement variables increases the r-squared to 0.0308, and the stepwise model achieves the highest value at 0.0411.

<div style="text-align: center;"> 
<img src="regression_images/linear_regression_comparison.png" width="60%"> 
<p>Table 6.4&nbsp;&nbsp;&nbsp;&nbsp;Comparison of linear regression models</p> </div>

Despite these incremental differences, all three models ultimately exhibit low r-squared. This is unsurprising in the context of short-form video performance, where much of viewer retention is shaped by factors not captured in metadata - such as creative quality, editing style, narrative pacing, emotional tone, and topic relevance. Completion outcomes are also influenced by platform recommendation systems, which shape exposure patterns in ways not observable in the dataset. Moreover, linear models impose additive and monotonic relationships that may be too restrictive for inherently nonlinear viewing behaviors. Together, these unobserved elements limit the explanatory power of linear models and help clarify why metadata-based regressions account for only a small fraction of the variation in completion rate.

#### **6.1.2.5 Regression Diagnostics**

To assess whether the stepwise model satisfies key regression assumptions, we examine the standard set of diagnostic plots. These plots provide insight into linearity, homoscedasticity, normality of residuals, and the influence of individual observations.

<div style="text-align: center;"> 
<p>Figure 6.1&nbsp;&nbsp;&nbsp;&nbsp;Diagnostic plots for the stepwise linear model</p> 
<img src="regression_images/diagnostic_checks.png" width="100%"> 
</div>

The Residuals vs Fitted and Scale–Location plots show slight changes in residual spread across fitted values, indicating mild heteroscedasticity and some non-linearity. The Q–Q plot reveals noticeable deviations in the tails from a normal distribution; however, this is also expected because Q–Q plots become highly sensitive with large sample sizes and can highlight even very small departures from normality. The Residuals vs Leverage plot shows a few moderately high-leverage points, but none appear strongly influential. 

Overall, the linear model remains adequate for exploratory purposes, but the diagnostics indicate that its assumptions are not fully satisfied and the linear specification may not fully capture the underlying patterns in the data. These limitations motivate the use of nonlinear approaches, such as smooth and wiggly GAM, to capture potential nonlinear patterns that the linear specification may not fully capture.

### **6.1.3 Nonlinear Modeling and Model Comparison**

Building on the predictors retained from the stepwise linear model, we extend the analysis to nonlinear specifications to assess whether more flexible functional forms can better capture patterns in completion_rate. In particular, we compare a standard linear model with two generalized additive models (GAMs) that allow nonlinear effects of the predictors.

#### **6.1.3.1 Train–Test Split for Nonlinear Models**

To evaluate predictive performance while avoiding overfitting, we randomly sampled 5,000 observations from the full dataset and applied the same preprocessing steps used in earlier models. The resulting dataset was then partitioned into an 80% training set and a 20% test set. All nonlinear models were fit on the training data and evaluated on the held-out test data.

#### **6.1.3.2 Model Specifications: Linear vs Smooth vs Wiggly GAM**

We estimate three models using the same predictor set identified through stepwise selection:

1. Linear Model — assumes additive, linear effects for all predictors.

2. Smooth GAM — allows predictors such as duration_sec to vary smoothly, capturing gentle nonlinear trends.

3. Wiggly GAM — uses a higher degree of smoothness flexibility, allowing complex and rapidly changing nonlinear shapes.

These models represent a progression from simple to increasingly flexible structures, enabling assessment of whether added nonlinearity improves fit or merely introduces noise.

#### **6.1.3.3 Visual Comparison of three models**

To illustrate differences in model behavior, we visualize fitted values against `duration_sec`, because it is a continuous predictor that allows the linear, smooth, and wiggly models to produce interpretable fitted curves. Variables such as `log_likes` or `has_emoji` are either discrete or less suitable for illustrating non-linear patterns.

<div style="text-align: center;"> 
<p>Figure 6.2&nbsp;&nbsp;&nbsp;&nbsp;Visual comparison of linear, smooth GAM, and wiggly GAM fits</p>
<img src="regression_images/nonlinear_comparison.png" width="100%"> 
 </div>

Across all three models, the downward association between video duration and completion_rate is clearly visible. The Linear Model produces a simple declining line, while the Smooth GAM allows gradual curvature but remains stable and consistent with overall trends. In contrast, the Wiggly GAM shows highly fluctuating and unstable fits, suggesting overfitting to noise rather than capturing meaningful structure.

#### **6.1.3.4 Cross-Validation Performance and Visualization**

Although prediction is not the primary goal of our analysis, cross-validated RMSE provides a useful way to compare how differently structured models behave out of sample and whether added flexibility leads to more stable estimates. Rather than interpreting RMSE as a measure of forecasting accuracy, we use it here to assess the extent to which each model captures consistent patterns versus overfitting noise.

<div style="text-align: center;"> 
<p>Figure 6.3&nbsp;&nbsp;&nbsp;&nbsp;RMSE distributions for the linear and GAM models</p> 
<img src="regression_images/RMSE_distribution.png" width="100%"> 
</div>

The RMSE distributions show that the Linear Model and Smooth GAM produce similarly low and stable error levels across folds, indicating that their fitted relationships generalize reasonably well to held-out data. In contrast, the Wiggly GAM exhibits both higher RMSE values and substantially greater variability, showing the instability observed in its fitted curve. This pattern reinforces the conclusion that excessive model flexibility primarily captures sampling noise rather than meaningful structure in the relationship between duration and completion rate.

Overall, these results suggest that modest nonlinear flexibility can be informative, but highly wiggly specifications offer little additional insight and reduce model stability. This indicates that simpler models are better suited for summarizing the underlying relationships in the data and align well with the exploratory goals of Topic 1.

### **6.1.4 Conclusion of Regression Findings**

The regression analyses collectively provide a structured overview of how metadata features relate to short-form video completion rates. Across all model specifications, video duration consistently emerges as the strongest and most stable correlate of completion rate, exhibiting a negative association in all models. Engagement-related variables such as `log_likes` and `log_saves` also contribute meaningfully once introduced, suggesting that videos that prompt stronger viewer interaction tend to retain audiences slightly better. Creator-level factors, particularly `log_creator_avg`, show weaker and  inverse relationships, indicating that larger creators do not necessarily achieve higher completion rates.

Although adding predictors improves model fit incrementally, the overall explanatory power remains low, with the best linear model achieving an r-squared of approximately 0.041. This reflects the inherent limitation of metadata, as viewer retention is a complex outcome shaped by uncaptured features such as creative quality, narrative structure, editing choices, emotional resonance, and platform dynamics.

Nonlinear modeling further highlights these constraints. The Smooth GAM captures mild curvature but offers only marginal improvements over the linear specification, while the Wiggly GAM overfits and performs poorly out of sample. Therefore, these results show that more flexible functional forms do not substantially enhance predictive accuracy, reinforcing the conclusion that metadata explains only a small portion of completion behavior.

Overall, the regression analysis provides useful descriptive insight into how structural and engagement-related attributes relate to completion rate, but it also underscores the central role of unobserved creative and algorithmic factors. These findings motivate the exploration of richer feature sets or alternative modeling approaches in future work, while also providing a clear baseline for interpreting patterns in video performance.

## **6.2 Topic 2: Trend Prediction**

### **6.2.1 Purpose of the ML Analysis**

We applyed several algorthms designed to predict popularity trends for short-form videos. By analyzing extensive video data, we identify patterns in viral content and assess the breakout potential of new videos. By testing multiple algorithms, we identify the most accurate predictive model, providing data-driven support for content platforms, creators, and advertisers.

### **6.2.2 Data Preparation and Exploration**

#### **6.2.2.1 Dataset Characteristics**

We created a simulated dataset of 50,000 samples with the following characteristics:

- **Platform Distribution**: 60% TikTok, 40% YouTube

- **Content Categories**: Entertainment, Music, Sports, Education, Gaming

- **Creator Tiers**: Micro (50%), Mid (30%), Macro (15%), Star (5%)

- **Key Metrics**: Title length, text richness, comment rate, share rate, daily views

#### **6.2.2.2 Target Variable Construction**

We constructed an **Engagement Score** using the following weighted formula:

$$
\begin{aligned}
\text{Engagement Score} = & \left(\frac{\text{comment_rate}}{0.02}\right) \times 0.2 \\
                         & + \left(\frac{\text{share_rate}}{0.005}\right) \times 0.3 \\
                         & + \left(\frac{\text{views_per_day}}{50000}\right) \times 0.2 \\
                         & + \text{text_richness} \times 0.1 \\
                         & + \text{weekend_hashtag_boost} \times 0.1 \\
                         & + \text{creator_tier_bonus} \\
                         & + N(0, 0.2)
\end{aligned}
$$

Where the creator tier bonus is defined as:

$$
\text{creator_tier_bonus} = 
\begin{cases}
0.5 & \text{if Star} \\
0.3 & \text{if Macro} \\
0.1 & \text{if Mid} \\
0 & \text{if Micro}
\end{cases}
$$

Based on the Engagement Score, videos are classified into three trend categories:

- **Low Trend (0)**: Engagement Score ≤ 0.8

- **Medium Trend (1)**: 0.8 < Engagement Score ≤ 1.5  

- **High Trend (2)**: Engagement Score > 1.5

### **6.2.3 Feature Engineering**

We created three interaction features to capture complex relationships:

1. **Platform-Tier Interaction**: Combines platform type with creator tier
   
   $$
   \text{platform_tier} = \text{platform} \oplus \text{creator_tier}
   $$

2. **Region-Category Interaction**: Combines geographic region with content category
   
   $$
   \text{region_category} = \text{region} \oplus \text{category}
   $$

3. **Engagement Velocity**: Composite metric measuring virality speed
   
   $$
   \text{engagement_velocity} = \text{views_per_day} \times (\text{comment_rate} + \text{share_rate})
   $$

Where $\oplus$ denotes feature concatenation.

The final model utilizes 15 features including:

- 12 original features (platform, region, category, traffic_source, device_brand, creator_tier, title_len, text_richness, comment_rate, share_rate, views_per_day, weekend_hashtag_boost)

- 3 interaction features (platform_tier, region_category, engagement_velocity)

### **6.2.4 Data Preprocessing**

#### **6.2.4.1 Data Splitting**

The dataset was randomly split into training (80%) and testing (20%) sets while maintaining proportional representation of each trend class in both subsets.

#### **6.2.4.1 Feature Transformation**

- **Numerical Features**: Standardized to zero mean and unit variance

- **Categorical Features**: Transformed using one-hot encoding

### **6.2.5 Model Definition**

We tested six machine learning models:

1. **Random Forest**: Ensemble of 200 decision trees with maximum depth 15

2. **Gradient Boosting**: 200 sequential trees with learning rate $\eta = 0.1$

3. **Extra Trees**: Extremely randomized trees variant with 200 estimators

4. **Neural Network**: Multi-layer perceptron with architecture $[100, 50]$ neurons

5. **Support Vector Machine**: RBF kernel with regularization $C = 1.0$

6. **Logistic Regression**: Linear classifier with one-vs-rest strategy

### **6.2.6 Model Training and Evaluation**

#### **6.2.6.1 Evaluation Metrics**

Models were evaluated using multiple metrics:

- **Accuracy**

- **Macro-average F1 Score**

- **Weighted-average F1 Score**

- **AUC-OVR**

- **5-Fold Cross-Validation Accuracy**

#### **6.2.6.2 Training Process**

All models were trained using identical preprocessing pipelines to ensure fair comparison, with performance evaluated on the held-out test set.

### **6.2.7 Results Comparison**

#### **6.2.7.1 Performance Ranking**

Gradient Boosting demonstrated superior performance across all metrics:

![](classification_model_comparison.png)

### **6.2.8 Best Model Analysis**

The Gradient Boosting model achieved the following confusion matrix:

![](classification_confusion_matrix.png)

The model demonstrates strong performance distinguishing Low from High trends but shows some confusion at the Medium-High boundary, with 206 High-trend videos misclassified as Medium.

### **6.2.9 Feature Importance Analysis**

The Gradient Boosting model revealed the following feature importance ranking:

![](classification_feature_importance.png)

Analysis confirms user interaction metrics (particularly sharing and commenting) are the primary predictors of video trends, accounting for 65% of the model's decision-making.

# **7. Discussion**

## **7.1 Discussion of Completion Rate**

### **7.1.1 Clear Patterns Identified**

The regression results reveal several clear patterns in the relationship between video features and view completion rates.

#### **7.1.1.1 Video Duration: The Dominant Factor**

Among all models, **video length consistently has the strongest influence**:
- Longer videos tend to have **lower view completion rates**
- Aligns with expectations for short-video platforms
- Viewers prefer quick and easy-to-understand content

#### **7.1.1.2 Interaction Metrics: Positive Correlates**

Interaction metrics such as **likes and saves** are positively correlated with view completion rates:
- Videos that retain viewers tend to generate more interaction
- Suggests a bidirectional relationship between retention and engagement

### **7.1.2 Surprising Findings**

More surprisingly, an unexpected finding is that **creator-level attributes**, including average historical views, have a **weak and even negative correlation** with view completion rates.

**Implications:**
- Having a large audience does not guarantee high retention rates
- **Content features may be more important than creator scale**
- Challenges assumptions about influencer effectiveness

### **7.1.3 Model Performance: The Low R-Squared Problem**

The **consistently low R-squared value across all models** indicates that these predictors explain only a **small portion of the variation** in view completion rates.

Adding more predictors provides **limited improvement**, further suggesting that metadata only captures some of the potential factors influencing viewer retention.

**What's Missing:**
- Creative quality and production value
- Narrative structure and pacing
- Emotional resonance
- Visual aesthetics
- Algorithmic exposure patterns

### **7.1.4 Broader Insights**

#### **7.1.4.1 Content Quality Over Creator Identity**

View completion rates appear to depend more on **video-level quality**, such as:
- Content design
- Thematic appeal
- Execution quality

Rather than:
- Creator's identity
- Posting time
- Follower count

#### **7.1.4.2 The Metadata Limitation**

The low R-squared value confirms that **most determinants of retention**—including creative quality and algorithmic exposure—are **not reflected in the existing variables**.

#### **7.1.4.3 Non-Linear Models Don't Help**

The non-linear model shows **little improvement over the linear model**, suggesting that the primary limitation lies in:
- **The lack of relevant content-level information**
- NOT the functional form of the model

### **7.1.5 Conclusion**

In conclusion, **metadata can reveal some meaningful patterns**, but it can only explain a **small fraction of audience behavior** on short video platforms.

**Key Takeaway:** The challenge is not about finding the right model—it's about the fundamental limits of what observable metadata can capture about inherently creative and algorithmic processes.

## **7.2 Discussion of Trend Prediction**

### **7.2.1 Expected vs. Unexpected Findings**

#### **7.2.1.1 Gradient Boosting Performance: As Expected**

The superior performance of **Gradient Boosting was anticipated**, given its proven effectiveness in handling structured data with complex feature interactions.

#### **7.2.1.2 Magnitude of Difference: Beyond Expectations**

What **exceeded expectations** was the **magnitude of difference** between models:
- Gradient Boosting achieved **3.2% higher accuracy** than its nearest competitor
- Demonstrates clear superiority for this task

### **7.2.2 System Effectiveness**

Our research results show that the proposed trend prediction system **effectively captures meaningful behavioral patterns** in short video participation.

#### **7.2.2.1 Clear Category Separation**

**High, Medium, and Low Trend categories** show clear and consistent separations in key indicators:
- Views
- Likes
- Comments
- Shares
- Saves

#### **7.2.2.2 Alignment with Expectations**

This model is **highly consistent with expectations**:
- The higher the trend of a video, the more user engagement it naturally accumulates
- This accelerates visibility in the platform's recommendation ecosystem

### **7.2.3 Validation of Feature Design**

#### **7.2.3.1 Distribution of Engagement Scores**

The distribution of engagement scores further validates the **reliability of the constructed labels**:
- Each category forms a **distinct group**
- Indicates that our feature design has successfully captured the **underlying drivers of viral spread**

#### **7.2.3.2 Feature Importance Findings**

Analysis confirms that **user interaction metrics** (particularly sharing and commenting) are the **primary predictors** of video trends, accounting for:
- **65% of the model's decision-making**

This validates the theoretical foundation that viral content is driven by active user participation.

### **7.2.4 Practical Implications**

#### **7.2.4.1 For Content Creators**

The model provides actionable insights:
- Focus on **share-worthy content** (highest feature importance)
- Optimize for **comment generation** through engaging hooks
- Consider **platform-specific strategies** (TikTok vs. YouTube Shorts)

#### **7.2.4.2 For Platforms**

Understanding trend drivers enables:
- Better recommendation algorithm design
- Content moderation prioritization
- Creator support and guidance

#### **7.2.4.3 For Marketers**

Trend prediction allows:
- Early identification of viral opportunities
- Strategic partnership timing
- ROI optimization for influencer campaigns

---

# **8. Limitations and Future Work**

## **8.1 Limitations of Completion Rate Analysis (Topic 1)**

### **8.1.1 Heterogeneity Problem: One Model for All?**

A major limitation of regression analysis is that it **models all video data as a single merged dataset** without distinguishing between:
- Different platforms (TikTok vs. YouTube Shorts)
- Languages and cultural contexts
- Content categories (music, education, gaming)

#### **8.1.1.1 Why This Is Problematic**

Because short videos vary greatly in:
- Purpose
- Audience expectations
- Cultural context

**Merging all observations introduces significant heterogeneity**, which:
1. **Weakens true relationships** by averaging across disparate groups
2. **Makes some predictors insignificant** that might be strong within specific subgroups
3. **Masks confounding patterns** that may exist in specific subgroups

#### **8.1.1.2 The Large Sample Exacerbates the Problem**

The large sample size further exacerbates this issue by:
- Detecting statistically significant but practically meaningless effects
- Obscuring meaningful patterns within specific content types

### **8.1.2 The Low R-Squared Problem**

The consistently **low R-squared values** across all models indicate that the current set of metadata variables captures only a **small portion of the factors** influencing completion rate.

#### **8.1.2.1 Multiple Interpretations**

This weak model fit may reflect:
1. **The limits of metadata** — creative quality cannot be quantified through observable features
2. **The absence of important variables** — content-level or behavior-level variables not observable in the dataset

### **8.1.3 Recommendations for Future Research**

In future research, we should:

#### **8.1.3.1 Expand the Breadth of Observed Variables**

Include:
- Content-level features (visual quality, editing pace, music choice)
- Behavior-level features (scroll patterns, replays, pause behavior)
- Algorithmic features (recommendation score, position in feed)

#### **8.1.3.2 Use Stratified or Subgroup Analysis**

Better control the model by:
- **Modeling completion rate separately** by language or platform
- Determining whether observed correlations are **consistent across different content types**
- Identifying **confounding factors specific to each category**

**Example Applications:**
- Analyze whether duration effects differ for education vs. entertainment content
- Test whether creator tier matters more on TikTok than YouTube Shorts
- Examine whether emoji effects vary by audience age group

#### **8.1.3.3 Benefits of Stratification**

This approach can:
- **Identify category-specific patterns** masked in aggregate analysis
- **Control for confounding** by analyzing homogeneous subgroups
- **Provide more actionable insights** for content creators in specific niches

## **8.2 Limitations of Trend Prediction (Topic 2)**

Although the proposed system demonstrates **strong predictive ability** for short video trends, there are still some limitations providing opportunities for future enhancement.

### **8.2.1 Data Quality and Representativeness**

#### **8.2.1.1 Synthetic Data Limitations**

The dataset relies on **synthetic or proxy-generated behavioral patterns**, which means:
- The engagement relationships in model learning may not fully capture **platform-specific nuances**
- May miss **rapidly evolving user habits** in real-world environments
- Lacks the complexity of actual algorithmic amplification effects

### **8.2.2 Feature Completeness**

#### **8.2.2.1 Missing Multimodal Elements**

The current feature design mainly focuses on **structured metadata and digital interaction metrics**, but does not incorporate:

**Visual Features:**
- Video content quality
- Visual aesthetics
- Color schemes and composition

**Audio Features:**
- Music choice and tempo
- Audio quality
- Sound effects

**Text Semantics:**
- Title and description sentiment
- Topic modeling
- Semantic similarity to trending content

**Known Impact:** These multimodal elements are **known to have a significant impact on viral spread** but are not currently captured.

### **8.2.3 Temporal Dynamics**

#### **8.2.3.1 Static vs. Dynamic Modeling**

The **contemporaneity of social trends has been simplified**:
- These models handle **each sample independently**
- Do not model the **dynamic trend propagation over time**

**Missing Elements:**
- Early momentum signals
- Cascading sharing patterns
- Time-to-viral metrics
- Trend decay patterns

### **8.2.4 Model Architecture Opportunities**

#### **8.2.4.1 Current Limitations**

The current model uses traditional machine learning approaches, which may miss:
- Complex sequential dependencies
- Long-range feature interactions
- Hierarchical content structures

#### **8.2.4.2 Advanced Technologies for Enhancement**

Model performance can be further enhanced through:

**1. Transformer-Based Architectures**
- Better capture of sequential patterns
- Attention mechanisms for feature importance
- Pre-trained models for transfer learning

**2. Self-Supervised Representation Learning**
- Learn rich representations from unlabeled video data
- Capture latent features not in metadata
- Reduce dependence on labeled data

**3. Automatic Hyperparameter Optimization**
- More efficient model tuning
- Better generalization
- Reduced human bias in model selection

## **8.3 Future Work Directions**

### **8.3.1 Data Integration**

#### **8.3.1.1 Real Platform Data Streams**

Integrating **real platform data streams** would enable:
- Validation of findings on actual user behavior
- Capture of platform-specific algorithmic effects
- Real-time trend tracking

#### **8.3.1.2 Expanding the Feature Space**

Include:
- **Visual and NLP-based content signals** (scene detection, sentiment analysis)
- **User trajectory data** (viewing history, scroll patterns)
- **Network effects** (follower graphs, sharing cascades)

### **8.3.2 Methodological Enhancements**

#### **8.3.2.1 Time Series Prediction Methods**

Adopting **time series approaches** such as:
- **Sequence models** (LSTM, GRU) for temporal dependencies
- **Graph-based diffusion learning** for viral spread modeling
- **Survival analysis** for trend lifecycle prediction

#### **8.3.2.2 Multi-Task Learning**

Simultaneously predict:
- Trend label
- Completion rate
- Engagement metrics

This could capture shared underlying factors and improve overall performance.

### **8.3.3 Real-World Deployment**

#### **8.3.3.1 Real-Time Feedback Loop**

Deploying the model into a **real-time feedback loop** would:
- Enable continuous learning from new data
- Adapt to evolving platform dynamics
- Provide immediate value to stakeholders

#### **8.3.3.2 Production Constraints Evaluation**

Evaluating the model under **production constraints**:
- Latency requirements
- Computational costs
- Update frequency
- Model interpretability for non-technical users

### **8.3.4 Stakeholder Applications**

#### **8.3.4.1 For Content Creators**

Future systems could provide:
- Real-time trend prediction before publishing
- Optimization suggestions for title, hashtags, timing
- Competitive benchmarking

#### **8.3.4.2 For Marketers**

Enhanced capabilities for:
- Early identification of emerging trends
- Influencer selection and campaign timing
- ROI prediction and optimization

#### **8.3.4.3 For Recommendation Systems**

Integration with platform algorithms for:
- Better content discovery
- Personalized trend surfaces
- Diversity in trending content

## **8.4 Concluding Thoughts**

### **8.4.1 What We Learned**

**Completion Rate:**
- Metadata can identify structural patterns
- But cannot capture creative quality
- Descriptive insights are more valuable than predictive accuracy

**Trend Prediction:**
- Multi-metric engagement signals are more predictable
- Machine learning can effectively classify popularity levels
- Feature engineering is crucial for performance

### **8.4.2 The Path Forward**

The future of short-form video analytics lies in:
1. **Integrating multimodal data** (visual, audio, text)
2. **Modeling temporal dynamics** of trend propagation
3. **Deploying real-time systems** that learn continuously
4. **Balancing predictive power with interpretability**

By addressing these limitations and pursuing these directions, future work can **further verify the practical value** of trend prediction systems for content creators, marketers, and recommendation platforms.